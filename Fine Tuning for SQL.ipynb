{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-A6RQBDaa0p"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers -U\n",
        "!pip install accelerate -U\n",
        "!pip install trl\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "id": "ubFQn9mFawgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "fWiTr8J3a8_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "DATASET_NAME = \"ChrisHayduk/Llama-2-SQL-Dataset\"\n",
        "\n",
        "dataset = load_dataset(DATASET_NAME)"
      ],
      "metadata": {
        "id": "oqXvLPy1a_Yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_training_dataset = dataset[\"train\"]\n",
        "shuffled = full_training_dataset.shuffle()\n",
        "training_dataset = shuffled.select(range(1000))"
      ],
      "metadata": {
        "id": "8j_fb-h7cLgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bitsandbytes as bnb\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        "    bnb_4bit_quant_type=\"nf4\")\n"
      ],
      "metadata": {
        "id": "IYbtHeb5cakX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"NousResearch/Llama-2-7b-hf\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "model.config.use_cache = True\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "eAnYZUubdBxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_datapoint(x):\n",
        "  combined = x['input'] + x['output']\n",
        "  return tokenizer(combined, padding = True)\n",
        "\n",
        "training_dataset = training_dataset.map(construct_datapoint)"
      ],
      "metadata": {
        "id": "c9Aafsj5d0X7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_dataset)"
      ],
      "metadata": {
        "id": "NHQmEv3AfFK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=['q_proj', 'k_proj', 'down_proj', 'v_proj', 'gate_proj', 'o_proj', 'up_proj'],\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, peft_config) # all layers except attention are frozen\n",
        "\n",
        "generation_config = model.generation_config\n",
        "generation_config.max_new_tokens = 256\n",
        "generation_config.pad_token_id = tokenizer.eos_token_id\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "generation_config.temperature = 0.7\n",
        "generation_config.top_p = 0.9\n",
        "generation_config.do_sample = True"
      ],
      "metadata": {
        "id": "W0H9Etrjgr4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prompt):\n",
        "  generation_config.max_new_tokens = 20\n",
        "\n",
        "  encoded = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\").to(device)\n",
        "  with torch.inference_mode():\n",
        "    output_tokens = model.generate(\n",
        "        input_ids=encoded,\n",
        "        generation_config=generation_config,\n",
        "        repetition_penalty = 1.3\n",
        "    )\n",
        "\n",
        "  string_decoded = tokenizer.decode(output_tokens[0], clean_up_tokenization_spaces=True)\n",
        "  print(string_decoded)"
      ],
      "metadata": {
        "id": "edc7deVZidNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate('today I want to')"
      ],
      "metadata": {
        "id": "O2ZuhBmujV0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_arguements = transformers.TrainingArguments(\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    optim = \"paged_adamw_8bit\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.05,\n",
        "    output_dir=\"fine_tuning\"\n",
        ")\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=training_dataset,\n",
        "    args=training_arguements,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "\n",
        "model.config.use_cache = False"
      ],
      "metadata": {
        "id": "zf-MN24ijYat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "xXBYmdLwkTM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#API Key: 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"
      ],
      "metadata": {
        "id": "HvpZG4eVkpX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = dataset['eval'].shuffle()\n",
        "\n",
        "sample_sql_question = eval_dataset[0]['input']\n",
        "correct_answer= eval_dataset[0]['output']\n",
        "\n",
        "print('Query:',sample_sql_question)\n",
        "print('Answer:',correct_answer)"
      ],
      "metadata": {
        "id": "eYD9RcZ4mXJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(sample_sql_question)"
      ],
      "metadata": {
        "id": "IkfEln80mm4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lI7xojIisEfl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}